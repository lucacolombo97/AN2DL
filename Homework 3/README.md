**VQA**
For the VQA problem, our approach was to pass the two inputs, the image and the question, to two differents nets and then merge the two latent representations with concatenation to make the final prediction. For the image we tried different nets, and at the end InceptionResNetV2 resulted in the best performance. For the questions, first we embedded them and then we used a two layers LSTM to extract features. After the concatenation we used fully connected layers and a softmax for final prediction. We fine tuned various hyperparameters like embedding size, number of LSTM layers and dropout rate. We had to resize the images so that the entire training set could fit in our RAM.
